---
title: "dong_chris_housing"
author: "Chris Dong"
date: "September 20, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE, results = 'hide'}
knitr::opts_chunk$set(echo = TRUE)
```

Loading the data and any packages
```{r, }
options("max.print"=3)
suppressMessages(library(tidyverse))
suppressMessages(library(magrittr))
suppressMessages(library(leaps))
suppressMessages(library(VIM))
suppressMessages(library(car))
suppressMessages(library(Hmisc))
suppressMessages(library(glmnet))
house <- read_csv("housing.txt", col_types = cols())
names(house) <- tolower(names(house))
house0 <- house
```

Convert `mssubclass` to factor and check for `NA`s
```{r}
house$mssubclass <- factor(house$mssubclass)
house %>% sapply(function(x) sum(is.na(x))) %>% sort(decreasing = T)
```

Convert numeric variables that have `NA` to 0. Change `garageyrblt` to indicate whether or not the garage was built AFTER the house was built.
```{r}
house$masvnrarea[which(is.na(house$masvnrarea))] <- 0
house$bsmtfintype1[which(is.na(house$bsmtfintype1))] <- 0
house$bsmtfintype2[which(is.na(house$bsmtfintype2))] <- 0
house$garageyrblt <- (house$garageyrblt > house$yearbuilt) * 1
house$garageyrblt[is.na(house$garageyrblt)] <- 0
```

Impute the NA in `lotfrontage`, `electrical`  with K-Nearest Neighbors
```{r,warning = FALSE, message = FALSE}

k = round(sqrt(1460*.8) / 2)

house$lotfrontage <- kNN(house, variable = "lotfrontage",  k = k)$lotfrontage
house$electrical <- kNN(house, variable = "electrical",  k = k)$electrical
```

Convert all other `NA`s to "None"
```{r}
house[is.na(house)] <- "None"
```

Make a new variable, `remodel` that indicates whether or not remodeling took place. Remove the `yearremodadd` variable because it is no longer needed. Make a new variable `soldminusbuilt` that indicates the number of years that it took for the house to get sold after getting built.
```{r, warning = F}
house$remodel <- T
house[house$yearbuilt == house$yearremodadd,]$remodel <- F
house$remodel <- as.numeric(house$remodel)
house %<>% select(-yearremodadd) 

house$soldminusbuilt <- (house$yrsold - house$yearbuilt)
house %<>% select(-yrsold,-yearbuilt) 
```

Combine all of the porch variables into one. Remove `id` because it is obviously not important.
```{r}
house$porcharea <- with(house, openporchsf + enclosedporch +
    `3ssnporch` + screenporch)
house %<>% select(-id) 
```

Change `lotshape` to a boolean whether or not it is Regular.
```{r}
table(house$lotshape)
house$lotshape <- (house$lotshape == 'Reg') *1
```

Looking at the histogram of `mosold` we see many more houses being sold near summer time (and part of spring too) so we create a boolean. Most of the time, when we are creating a boolean, it is because it is insignificant otherwise.
```{r}
house %>% ggplot(aes(x=mosold)) + geom_histogram(binwidth = 1) + xlim(0,13)+
   xlab("Month Sold") +
  ylab("Frequency")
  
house$summertime <- (house$mosold %in% 5:7) * 1
```

The next part of the code was very time-consuming but here's the general outline:
It is similar to backwards selection but by hand and possibly more thorough because of the refactoring involved rather than simply removing it. 

1. Check the p-value and signifiance for a particlar variable. 
2. If the variable is numeric and significant, keep it. If the variable is categorical and all levels are significant, keep it. If only some levels are significant then try to bin the factors into smaller number of levels to try and make them statistically significant. If nothing can be done, then remove the variable.
3. Repeat the above steps for the rest of the variables. Each time we remove a variable, we re-run the lm model to check if the Adjusted R Squared changed significantly or not.
4. When we finish going through all the variables, there will be about 30 ones left to consider.

```{r}
house %<>% select(-mosold, -landcontour, -alley, -lotshape)
```

```{r}
house$lotconfig <- (house$lotconfig == "Inside")  * 1
house %<>% select(-lotconfig)
```

Here, we noticed `lotfrontage` became significant when we take the square root. We remove `1stflrsf`, `2ndflrsf`, `lowqualfinsf` because they make up the variable `grlivarea`. At first, we tried having all three of them and deleting `grlivarea` however we found that having just `grlivarea` performed better. We are deleting the porch variables because we have already aggregated them into `porcharea`.
```{r}
fullmodel <- lm(saleprice~sqrt(lotfrontage)+porcharea+.,data = house)
summary(fullmodel)$r.squared

house$condition1 <- relevel(factor(house$condition1), ref = "Norm")
house$condition2 <- relevel(factor(house$condition2), ref = "Norm")

house %<>% select(-roofstyle)
house %<>% select(-exterior2nd)

table(house$bldgtype)

house <- house %>% select(-`1stflrsf`, -`2ndflrsf`, -lowqualfinsf,
    -totalbsmtsf, -openporchsf, -enclosedporch, - `3ssnporch`,
    - screenporch, -garagearea)

house %>% group_by(salecondition) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))
house$salecondition <- (house$salecondition == "Normal") * 1 

house %>% group_by(saletype) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

house$newtype <- (house$saletype == 'New') * 1
house <- house %>% select(-saletype)

house$miscfeature <- (house$miscfeature != 'None') * 1
house %<>% select(-miscval, -miscfeature) 

house$paveddrive <- (house$paveddrive == 'Y') * 1
house %<>% select(-paveddrive) 

house$poolqc <- (house$poolqc !="None")*1
house$fence <- (house$fence !="None")*1
```

Here, I am changing the ordered factor into numeric. I want to make a correlation plot with every significant variable so I am converting all variables (as long as it makes sense) to numeric.
```{r}
house$garagecond <-  as.numeric(factor(house$garagecond, 
    levels = c("None","Po","Fa","TA","Gd","Ex"), labels = 0:5))
house$garagequal <-  as.numeric(factor(house$garagequal, 
    levels = c("None","Po","Fa","TA","Gd","Ex"), labels = 0:5))

house %<>% select(-fence,-poolqc,-garagecond)

house %>% group_by(garagefinish) %>% 
summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc)) %>% head(2)
house$garagefinish <-(house$garagefinish == "Fin") *1
house %<>% select(-garagefinish)
```

Here, `fireplacequ` and `fireplaces` are obviously correlated so I choose the one that seems to explain `saleprice` better. However, they both end up being insignificant.
```{r}
house$fireplacequ <-  as.numeric(factor(house$fireplacequ,
    levels = c("None","Po","Fa","TA","Gd","Ex"), labels = 0:5))
cor(house$saleprice,house$fireplacequ); cor(house$saleprice,house$fireplaces)
house %<>% select(-fireplacequ, -fireplaces)
```

```{r}
house %<>% select(-garageyrblt)
house$garagetype <- relevel(factor(house$garagetype), ref = "None")

house$functional <- (house$functional == "Typ") * 1

house$kitchenqual <-  as.numeric(factor(house$kitchenqual,
    levels = c("Po","Fa","TA","Gd","Ex"), labels = 1:5))
```

Similarly, `totrmsabvgrd` is highly correlated with `grlivarea` so I keep the better of the two.
```{r}
cor(house$totrmsabvgrd ,house$saleprice);cor(house$grlivarea ,house$saleprice)
house %<>% select(-totrmsabvgrd)
```

I try to combine all of the bath variables but they end up not being significant so I just remove them.
```{r}
table(house$fullbath)
house$bath <- house$fullbath + house$halfbath + house$bsmtfullbath + house$bsmthalfbath
house %<>% select(-fullbath,-halfbath, -bsmthalfbath, -bsmtfullbath)
house %<>% select(-bath)
```

```{r}
house %>% group_by(electrical) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc)) 
house$electrical <- (house$electrical == "SBrkr") * 1
house %<>% select(-electrical, -centralair)

house$heatingqc <- as.numeric(factor(house$heatingqc,
  levels = c("Po","Fa","TA","Gd","Ex"), labels = 1:5))
table(house$heatingqc)
house$heatingqc <- (house$heatingqc == 5) * 1

house %<>% select(-heating)

table(house$bsmtfintype1)

house$bsmtfintype1 <- as.numeric(factor(house$bsmtfintype1,
      levels = c("0","Unf","LwQ","Rec","BLQ","ALQ","GLQ"),
      labels = 0:6))
house$bsmtfintype2 <- as.numeric(factor(house$bsmtfintype2,
      levels = c("0","Unf","LwQ","Rec","BLQ","ALQ","GLQ"),
      labels = 0:6))
house$bsmtfintype1 <- house$bsmtfintype1 + house$bsmtfintype2
house %<>% select(-bsmtfintype1, -bsmtfintype2)


house$bsmtexposure <- relevel(factor(house$bsmtexposure), ref = "None")

table(house$bsmtexposure)

house %>% group_by(bsmtexposure) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

house$bsmtexposure <- (house$bsmtexposure == "Gd") * 1

house %>% group_by(bsmtcond) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

table(house$bsmtcond)

house$bsmtcond <- as.numeric(factor(house$bsmtcond,
      levels = c("None","Po","Fa","TA","Gd","Ex"),
      labels = 0:5))

house$bsmtqual <- as.numeric(factor(house$bsmtqual,
      levels = c("None","Po","Fa","TA","Gd","Ex"),
      labels = 0:5))
cor(house$bsmtcond,house$bsmtqual)
cor(house$bsmtcond,house$saleprice);cor(house$bsmtqual,house$saleprice)
house %<>% select(-bsmtcond)
house %<>% select(-bsmtqual)

table(house$foundation)

house %>% group_by(foundation) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

house$foundation <- (house$foundation == "PConc")*1

house$extercond <- as.numeric(factor(house$extercond,
      levels = c("Po","Fa","TA","Gd","Ex"),
      labels = 1:5))
house$exterqual <- as.numeric(factor(house$exterqual,
      levels = c("Po","Fa","TA","Gd","Ex"),
      labels = 1:5))
cor(house$extercond,house$exterqual)

house$masvnrtype <- relevel(factor(house$masvnrtype), ref = "None")

table(house$masvnrtype)

house$masvnrtype <- (house$masvnrtype != "None") * 1
```

Boolean whether or not housestyle is either `2Story` or `2.5Fin`.
```{r}
table(house$housestyle)

house %>%  group_by(housestyle) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

house$housestyle <- (house$housestyle == "2Story" | 
                    house$housestyle == "2.5Fin")*1

table(house$bldgtype)

house %>%  group_by(bldgtype) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

house$bldgtype <- (house$bldgtype == "1Fam" | house$bldgtype == "2FmCon") * 1
house %<>% select(-bldgtype)

table(house$landslope)

house$landslope <- (house$landslope == "Gtl") * 1
house %<>% select(-landslope)

table(house$utilities)
house %<>% select(-utilities, -street)

house %>%  group_by(mszoning) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

table(house$mszoning)

house$mszoning <- relevel(factor(house$mszoning), ref = "RL")

house %<>% select(-mszoning)

house %>%  group_by(mssubclass) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

house %<>% select(-mssubclass, -lotfrontage, -porcharea, -extercond,-foundation,
                  -exterior1st)


house %>%  group_by(condition1) %>% summarise(avgprc = median(saleprice)) %>% arrange(desc(avgprc))

house$condition1 <- (house$condition1 == "Artery" | house$condition1 =="Feedr"|
  house$condition1 == "RRAe")*1
house$condition2 <- (house$condition2 == "PosN") * 1

cor(house$garagequal, house$garagecars)

house %<>% select(-garagequal)

fullmodel <- lm(saleprice~.,data = house)
summary(fullmodel)
```

Checking multicollinearity. Looks good. For the generalized variance inflation factor (normalized by the degree of freedom), everything except one is less than 2. 
```{r}
vif(fullmodel)
```

Interestingly, `soldminusbuilt` which is `yrsold` - `yearbuilt` becomes insignificant in this smaller model with only the best predictors
```{r}
house_numeric <- house[,sapply(house,function(x) is.numeric(x))]
house_numeric %<>% select(saleprice, everything())
bestpredictors <- names(house_numeric)[sapply(house_numeric, 
function(x) abs(cor(house_numeric$saleprice, x))) >= 0.5][-1]

bestpredictors <- bestpredictors[-6]

bestmodel <- lm(saleprice~overallqual + exterqual + grlivarea + 
    kitchenqual + garagecars + neighborhood, data = house)

summary(bestmodel)$r.squared
```

Subset with only best predictors
```{r}
housesubset <- house %>% select(bestpredictors)
```

So, 6 variables capture `r summary(bestmodel)$r.squared` of the variation in sale price for our model.

Checking assumptions.

```{r}
cor(housesubset)
vif(bestmodel)

par(mfrow=c(2,4))
qqnorm(housesubset$grlivarea); qqline(housesubset$grlivarea)
qqnorm(log(housesubset$grlivarea)); qqline(log(housesubset$grlivarea))

qqnorm(house$saleprice); qqline(house$saleprice)
qqnorm(log(house$saleprice)); qqline(log(house$saleprice))
```

```{r}
bestmodel2 <- lm(log(saleprice)~overallqual  + exterqual +  log(grlivarea) + 
    kitchenqual + garagecars + neighborhood, data = house)
summary(bestmodel2)
```

`exterqual` becomes insignificant once we take the log of the response variable
```{r}
bestmodel3 <- lm(log(saleprice)~overallqual  +  log(grlivarea) + 
    kitchenqual + garagecars + neighborhood, data = house)
summary(bestmodel3)$r.squared
```

Check for influence points
```{r}
infm <- influence.measures(bestmodel3)
which(apply(infm$is.inf,1,any)) #influential observations
summary(infm)

plot(rstudent(bestmodel3) ~ hatvalues(bestmodel3))
#install.packages("olsrr")
suppressMessages(library(olsrr))
influence <- ols_dffits_plot(bestmodel3)
```

Let's examine Observation # 1299, and 524

```{r}
house[1299,] %>% View()
house[542,] %>% View()

bestmodel4 <- lm(log(saleprice)~overallqual  +  log(grlivarea) + 
    kitchenqual + garagecars + neighborhood, data = house[c(-1299,-542),])
summary(bestmodel4)$r.squared

```

By just removing two points, our Adjusted R-squared went from `r summary(bestmodel3)$adj.r.squared` to `r summary(bestmodel4)$adj.r.squared`

Let's see what happens if we simply remove the observations.
```{r}

influenceindex <- unlist(influence$outliers[1])

bestmodelnoinfluence <- lm(log(saleprice)~overallqual  +  log(grlivarea) + 
    kitchenqual + garagecars + neighborhood, data = house[-influenceindex,])
summary(bestmodelnoinfluence)$r.squared

```

We see that our Adjusted R-squared went from `r summary(bestmodel4)$adj.r.squared` to 
`r summary(bestmodelnoinfluence)$adj.r.squared` after removing ALL the influence points.


```{r}
house2 <- house
house2[influenceindex, ]$saleprice <- NA
house2$saleprice <- kNN(house2, variable = "saleprice",  k = k)$saleprice
```

```{r}
bestmodelimputeinfluence <- lm(log(saleprice)~overallqual  +  log(grlivarea) + 
    kitchenqual + garagecars + neighborhood, data = house2)
summary(bestmodelimputeinfluence)$r.squared
```

Let's try our model with all of the relevant variables. First, we notice that the R squared improves by taking the log of `saleprice`, `lotarea`, `grlivarea` and the square root of `bsmtfinsf1`. We also notice that `housestyle` and `masvnrtype` is no longer significant so we remove them.
```{r}
model31var <- lm(log(saleprice) ~ log(lotarea) + 
                   sqrt(bsmtfinsf1)+log(grlivarea)+., data = house)
summary(model31var)$r.squared
```

Accounting for outliers in the full model through imputation

```{r}
model31varimpute <- lm(log(saleprice) ~ log(lotarea) + 
              sqrt(bsmtfinsf1)+log(grlivarea)+., data = house2)
summary(model31varimpute)$r.squared
```

We can try removing the outliers, which improved the R squared by a lot. Now, we can test some interaction terms.

```{r}
model31varremove <- lm(log(saleprice) ~ log(lotarea) + 
              sqrt(bsmtfinsf1)+log(grlivarea)+., data = house2[-influenceindex,])
summary(model31varremove)$r.squared
```

I remove some variables found to be insignificant.
```{r}
house3 <- house2 %>% select(-condition2,-roofmatl,-garagetype,-poolarea,-remodel)
```

Remove `exterqual`
```{r}
house4 <- house3 %>% select(-exterqual)
```

#FINAL MODEL
I test the multicollinearity, significance of variables in the model, normality for our final model.
```{r}
endmodel <- lm(log(saleprice) ~ log(lotarea) + 
              sqrt(bsmtfinsf1)+log(grlivarea) +  . -
                lotarea - bsmtfinsf1 - grlivarea,
              data = house4[-influenceindex,])
vif(endmodel)
options(max.print=999)
summary(endmodel)
ks.test(endmodel$residuals, pnorm, mean(endmodel$residuals),
        sd(endmodel$residuals))
qqnorm(endmodel$residuals); qqline(endmodel$residuals)
```

Checking with LASSO if any variables to remove. Although LASSO recommends to delete `bsmtunsf` and `bedroomabvgr`, removing them lowers the R squared so I will keep them. Many of the neighborhoods are in fact significant so I will leave the non-significant levels in the model anyway.
```{r}
lassorefactor <- function(){
  
 x <- model.matrix(saleprice ~ ., data = house4)[,-1]
 y <- house$saleprice
 train <- sample(1:nrow(x), nrow(x) / 2)
 test <- (-train)
 y.train <- y[train]
 y.test <- y[test]
 grid.lambda <- 10^seq(10, -2, length = 100)
 lasso.model <- glmnet(x, y, alpha = 1, lambda = grid.lambda)
 set.seed(1)
 cv.out <- cv.glmnet(x[train,], y.train, alpha = 1)
 best.lambda <- cv.out$lambda.min
 lasso.pred <- predict(lasso.model, s = best.lambda, newx = x[test,])
 mspe.lasso <- mean((lasso.pred - y.test)^2)
 final.model <- glmnet(x, y, alpha = 1, lambda = best.lambda)
 c <- coef(final.model)
 ind <- which(c==0)
 variables <- row.names(c)[ind]
 return(variables)
}

lassorefactor()
```

Thus, our final model includes the following variables:

```{r}
names(house4)
```

```{r}
signif_var <- house4 %>% select(-neighborhood) %>% 
  sapply(function(x) abs(cor(x,house4$saleprice)))

signif_var[signif_var >= 0.5]
summary(lm(log(saleprice)~log(grlivarea) +kitchenqual +garagecars + soldminusbuilt + overallqual, data = house4))
```
# Part I: Explanatory Modeling

#*TASK 1*
The five most relevant features that are most relevant in determining a house's sale price are `overallqual`, `grlivarea`, `kitchenqual`, `garagecars`, and
`soldminusbuilt`. The fifth variable, `soldminusbuilt` is equal to `yearsold` - `yearbuilt`.

#*TASK 2*
```{r}
morty<- read_csv("Morty.txt", col_types = cols())
```

#*Function to transform TEST DATA accordingly. Please run the function transform() and provide the data frame to the argument*

```{r}
transform <- function(df){
  names(df) <- tolower(names(df))

  df[is.na(df)] <- "None"
  df$soldminusbuilt <- (df$yrsold - df$yearbuilt)
  df$summertime <- (df$mosold %in% 5:7) * 1
  df$newtype <- (df$saletype == 'New') * 1
  
  df %<>% select(intersect(names(df), names(house4)))
  
  df$condition1 <- (df$condition1 == "Artery" | 
      df$condition1 =="Feedr"| df$condition1 == "RRAe")*1
  
  df$housestyle <- (df$housestyle == "2Story" | 
                    df$housestyle == "2.5Fin")*1

  df$masvnrtype <- (df$masvnrtype != "None") * 1
  df$bsmtexposure <- (df$bsmtexposure == "Gd") * 1
  
  df$heatingqc <- as.numeric(factor(df$heatingqc,
  levels = c("Po","Fa","TA","Gd","Ex"), labels = 1:5))
  
  df$kitchenqual <-  as.numeric(factor(df$kitchenqual,
    levels = c("Po","Fa","TA","Gd","Ex"), labels = 1:5))
  
  df$functional <- (df$functional == "Typ") * 1
  df$salecondition <- (df$salecondition == "Normal") * 1 
  return(df)
}
morty2 <- transform(morty)
```

`morty2` is our transformed data. Note that it only has 25 variables
```{r}
confmorty <- exp(predict(endmodel, morty2, interval = "confidence", level = 0.95)) 
confmorty %>% knitr::kable()
```

```{r}
morty_stat <- as.numeric(unlist(morty2))
names(morty_stat) <- names(morty2)
mean_stat <- sapply(house4, function(x) round(mean(x)))

morty_stat
mean_stat

(improve <- house4 %>% select(-neighborhood,-saleprice, -soldminusbuilt) %>%  sapply(function(x) abs(cor(x, house4$saleprice))) %>% sort(decreasing = T) )

```

`overallqual` and `kitchenqual` are in the top 3 for correlation with saleprice. `grlivarea` is difficult/nearly impossible to improve so we will move on to the next variable. `masvnrarea` and `heatingqc` are fairly close. We see that Morty already has the highest `heatingqc` possible so `masvnrarea` should be considered.

*Conclusion:* Morty should try to improve the `overallqual`, which is the overall material and finish of the house. This may mean repainting some areas on the house to make it look nicer. Morty currently has a rating of 5 out 10 (average rating is 6 out of 10) so there is definitely room for improvement. Next, Morty should improve `kitchenqual`, which is kitchen quality. Maybe, there can be some remodeling done or fixing anything that is either old, or possibly broken. Morty has a rating of 3 out of 5 compared to the average rating of 4 out of 5. Finally, he can increase `masvnrarea`. He currently does not have a masonary veneer so he can consider building one because he might be able to make a profit from it.

We believe that Morty can sell his house for a *maximum* of `r prettyNum(round(confmorty[3], digits=2), big.mark = ",")`. The 95 $\%$ confidence interval goes from `r prettyNum(round(confmorty[1], digits=2), big.mark = ",")` to `r prettyNum(round(confmorty[3], digits=2), big.mark = ",")` with an average of `r prettyNum(round(confmorty[2], digits=2), big.mark = ",")`.

# Part II Predictive Modeling

#### Ordinary Least Squares
```{r}
set.seed(1)
train <- sample(nrow(house)*.8)
test <- (-train)
housetrain <- house4[train,]
housetest <- house4[test,]

OLS_train <- lm(log(saleprice) ~ log(lotarea) + 
              sqrt(bsmtfinsf1)+log(grlivarea) +  . -
                lotarea - bsmtfinsf1 - grlivarea,
              data = housetrain[-influenceindex,])
OLS_predict <- exp(predict(OLS_train, housetest, 
      interval = "prediction", level = 0.95, type = "response"))
prettyNum(mean((OLS_predict[,1] - housetrain$saleprice)^2), big.mark = ",")

```

```{r}
GLS_train <- glm(log(saleprice) ~ log(lotarea) + 
              sqrt(bsmtfinsf1)+log(grlivarea) +  . -
                lotarea - bsmtfinsf1 - grlivarea,
              data = housetrain[-influenceindex,])
GLS_predict <- exp(predict(OLS_train, housetest, 
      interval = "prediction", level = 0.95, type = "response"))
prettyNum(mean((GLS_predict[,1] - housetrain$saleprice)^2), big.mark = ",")
```

#### Define the function to generate models for ridge, lasso and elastic net
```{r}
model_func <- function(input_data, input_alpha){
set.seed(1)
x <- model.matrix(saleprice ~ ., data = input_data)[,-1]
y <- house$saleprice
train <- sample(nrow(house)*.8)
test <- (-train)
y.train <- y[train]
y.test <- y[test]
grid.lambda <- 10^seq(10, -2, length = 100)
model.train <- glmnet(x[train, ], y.train, alpha = input_alpha, lambda = grid.lambda)
set.seed(1)
cv.out <- cv.glmnet(x[train,], y.train, alpha = input_alpha)
best.lambda <- cv.out$lambda.min
pred <- predict(model.train, s = best.lambda, newx = x[test,])
mspe <- mean((pred - y.test)^2)
final.model <- glmnet(x, y, alpha = input_alpha, lambda = best.lambda)
c <- coef(final.model)
return(c(mspe, final.model))
}
```

#### Ridge regression model, $\lambda$ set at 0
```{r}
ridge_result <- model_func(house4,0)
ridge_mspe <- ridge_result[1]
prettyNum(ridge_mspe, big.mark = ",")
```

#### lasso regression model, lambda set at 1
```{r}
lasso_result <- model_func(house4,1)
lasso_mspe <- lasso_result[1]
prettyNum(lasso_mspe, big.mark = ",")
```


####elastic net regression, lambda set at 0.5
```{r}
elastic_result <- model_func(house4,0.5)
elastic_mspe <- elastic_result[1]
prettyNum(elastic_mspe, big.mark = ",")
```

$\lambda$ is chosen to determine whether we are performing Ridge ($\lambda = 0$ ), Lasso ($\lambda = 1$), Elastic Net ($\lambda = 0.5$). The tuning parameters in the respective models is chosen via cross validation after trying 100 different ones.
```{r}
help(cv.glmnet)
```

**Justification**

Our ridge model performed the best and has the lowest MSPE. This makes sense, given that our data is very sparse, containing many zeros.

```{r}
countzero <- function(x){
  sum(x==0)
}
sapply(house4, function(x) countzero(x))
```

Many of these are boolean variables, but we can see that `masvnrarea`, `bsmtfinsf`, `bsmtfinsf2`, and `bsmtunsf` all have zeros. We chose all of these variables because we found them to be statistically significant in our model.

```{r}
house4 %>% select(-neighborhood) %>% sapply(function(x) abs(cor(x, house4$saleprice))) %>% sort(decreasing = T)
```

Some variables have more impact than others but nevertheless they are statistically significant in our model so we keep them. Three of these variables are generated from other variables. We created `summertime` partly because of common sense and after plotting the distribution of houses being sold by month, we saw a peak in the summer months. This makes sense practically because people tend to have more time during the summer and thus are more likely to buy a house. Secondly, we created `soldminusbuilt` because we felt that the difference between `yearsold` and `yearbuilt` is more useful together rather than seperately. The third variable we created is a boolean for `saletype` to indicate a house that was "just constructed and sold", which from a common sense perspective, can make the house go much higher. Many of the variables are condensed into smaller levels. Many levels have very few observations so we feel they are not significant enough to have their own level. This helps to prevent overfitting when predicting new values. We chose to not have too many variables in our model to also prevent overfitting. We confirmed the validity of our variables through LASSO regression. Lasso didn't really eliminate any variables, which supports the statistical signifiance of our predictors. 

**Exploratory Data Analysis**
```{r}
p1 <- house4 %>% ggplot(aes(x=grlivarea, y = saleprice, 
                      color = factor(kitchenqual))) + geom_point(alpha = 0.5) +
  xlab("Above grade (ground) living area square feet") +
  ylab("Price of the house") + scale_y_continuous(label=scales::comma) +
  labs(colour = "Kitchen Quality") +
  theme(legend.title = element_text(size = 10, face = "bold")) 
p2 <- house4 %>% ggplot(aes(x=log(grlivarea), y = log(saleprice), 
                      color = factor(kitchenqual))) + geom_point(alpha = 0.5) +
  xlab("Above grade (ground) living area square feet") +
  ylab("Price of the house") + scale_y_continuous(label=scales::comma) +
  labs(colour = "Kitchen Quality") +
  theme(legend.title = element_text(size = 10, face = "bold")) 
library(grid)
library(gridExtra)
grid.arrange(p1,p2,ncol=1)
```

```{r}
house0%>% ggplot(aes(x=yearbuilt, y = saleprice, 
                      color = factor(roofstyle))) + geom_point(alpha = 0.5) +
  xlab("Built Year") +
  ylab("Price of the house") + scale_y_continuous(label=scales::comma) +
  labs(colour = "Type of Roof") +
  theme(legend.title = element_text(size = 10, face = "bold")) 
```


```{r}
ggsave("plot2.png")
```

Getting all of the numeric variables.
```{r, fig.width = 12, fig.height = 11}

house_numeric <- house4[,sapply(house4,function(x) is.numeric(x))]

house_numeric %<>% select(saleprice, everything())
#install.packages("ggcorrplot")

library(ggcorrplot)

cor_matrix <- cor(house_numeric) 

ggcorrplot(cor_matrix, type = "lower", outline.col = "white", insig = "blank")

```